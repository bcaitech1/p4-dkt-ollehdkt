model : lgbm   # {lstm, lstmattn, bert, lgbm, lstmroberta, lastquery, saint, lstmalbertattn}
 
# (비일반화) : base feature만 사용
# - lstm,lstmattn,bert
# (일반화) : 추가한 컬럼까지 범주형으로 사용

wandb :
    using: True
    project: DKT

    ## 자신의 wandb 아이디를 적어주세요
    entity: vail131
    tags: 
        - baseline

##main params
task_name: lgbm_jjin_final_use_croNwrongT
seed: 42
device: cuda

data_dir: /opt/ml/input/data/train_dataset
file_name: train_time_finalfix.csv
test_file_name: test_time_finalfix.csv

asset_dir: asset/
model_dir: models/
output_dir: output/

max_seq_len: 128
num_workers: 1

##K-fold params
use_kfold : True #n개의 fold를 이용하여 k-fold를 진행한다.
use_stratify : True
n_fold : 5
split_by_user : True #k-fold를 수행할 dataset을 user 기준으로 split

##모델
hidden_dim : 128
n_layers : 2
n_heads : 2
drop_out: 0.2

#train
n_epochs: 15
batch_size: 64
lr: 0.0001
clip_grad : 10
patience : 5
log_steps : 50
split_ratio : 0.8

cont_cols : 1
#중요
optimizer : adamW
scheduler: plateau
   
   
#use only in lgbm
lgbm:
    model_params: {
                    'objective': 'binary', # 이진 분류
                    'boosting_type': 'gbdt',
                    'metric': 'auc', # 평가 지표 설정
                    'feature_fraction': 0.4, # 원래 0.8 피처 샘플링 비율
                    'bagging_fraction': 0.6, # 원래 0.8 데이터 샘플링 비율
                    'bagging_freq': 1,
                    'n_estimators': 10000, # 트리 개수
                    'early_stopping_rounds': 100,
                    'seed': 42,
                    'verbose': -1,
                    'n_jobs': -1,
                    }
    
    verbose_eval : 100 #ori 100
    num_boost_round : 500
    early_stopping_rounds : 100
 

## LGBM feature enginnering 용 args
make_sharing_feature : True #extract statistics featrue from train + test(except last rows)
use_test_data : True #use test_data for train

# distance 피처 쓰기
use_distance : False