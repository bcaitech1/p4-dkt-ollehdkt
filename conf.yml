model : lstm    # {lstm, lstmattn, bert, lgbm, lstmroberta, lastquery, saint}

wandb :
    using: False
    project: DKT

    ## 자신의 wandb 아이디를 적어주세요
    entity: pomdeyo
    tags: 
        - baseline

##main params
task_name: target_check
seed: 42
device: cuda

data_dir: /opt/ml/input/data/train_dataset
file_name: train_data.csv
test_file_name: test_data.csv

asset_dir: asset/
model_dir: models/
output_dir: output/

max_seq_len: 20
num_workers: 1

##K-fold params
use_kfold : True #n개의 fold를 이용하여 k-fold를 진행한다.
use_stratify : False
n_fold : 5
split_by_user : False #k-fold를 수행할 dataset을 user 기준으로 split

##모델
hidden_dim : 128
n_layers : 4
n_heads : 2
drop_out: 0.2

#train
n_epochs: 200
batch_size: 64
lr: 0.0001
clip_grad : 10
patience : 20
log_steps : 50
split_ratio : 0.7

#중요
optimizer : adamW
scheduler: linear_warmup
   
   
#use only in lgbm
lgbm:
    model_params: {
                    'objective': 'binary', # 이진 분류
                    'boosting_type': 'gbdt',
                    'metric': 'auc', # 평가 지표 설정
                    'feature_fraction': 0.8, # 원래 0.8 피처 샘플링 비율
                    'bagging_fraction': 0.8, # 원래 0.8 데이터 샘플링 비율
                    'bagging_freq': 1,
                    'n_estimators': 10000, # 트리 개수
                    'early_stopping_rounds': 100,
                    'seed': 42,
                    'verbose': -1,
                    'n_jobs': -1,
                    }
    
    verbose_eval : 100 #ori 100
    num_boost_round : 500
    early_stopping_rounds : 100
 

## LGBM feature enginnering 용 args
make_sharing_feature : True #extract statistics featrue from train + test(except last rows)
use_test_data : True #use test_data for train
