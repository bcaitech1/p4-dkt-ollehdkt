work : 'train_and_inference' # 'train_and_inference','tuning', 'server', 'test'

# 모델 : 일반화된 모델을 사용하고 싶으면, g를 붙일 것
# 미 일반화된 모델 : LSTM, Bert
model : gpt2 # {lstm, lstmattn, bert, lgbm, lstmroberta, lastquery_pre,lastquery_post, saint, lstmalbertattn,gLSTMConvATTN, gsaint, gsaintplus}
 
# (비일반화) : base feature만 사용
# - lstm,lstmattn,bert
# (일반화) : 추가한 컬럼까지 범주형으로 사용

wandb :
    using: False
    project: DKT-slidding-gpt2

    ## 자신의 wandb 아이디를 적어주세요
    entity: sangki930
    tags: 
        - baseline

##main params
task_name: model-gpt2-01
seed: 42
device: cuda

data_dir: /opt/ml/input/data/train_dataset
file_name: train_data.csv
# file_name : time_train.csv
# file_name : train_inter_time (1).csv
# file_name : train_time_finalfix.csv
# file_name : JH_train_v3.csv
test_file_name: test_data.csv
# test_file_name : time_test.csv
# test_file_name : test_inter_time (1).csv
# test_file_name : test_time_finalfix.csv
# test_file_name : JH_test_v3.csv

asset_dir: asset/
model_dir: models/
output_dir: output/

max_seq_len: 128
num_workers: 1

##K-fold params
use_kfold : True #n개의 fold를 이용하여 k-fold를 진행한다.
use_stratify : False
n_fold : 5
split_by_user : False #k-fold를 수행할 dataset을 user 기준으로 split

##모델
hidden_dim : 256
n_layers : 4
n_heads : 4
drop_out: 0.2

# 연속형 컬럼에 마스크를 적용할 것 인지
use_mask : True
use_roll : False

# Augmentation
window : True # 슬라이딩 윈도우를 사용할 것인지
stride : 128
shuffle : False
shuffle_n : 3

# transformers
concat_reverse : False

#train
n_epochs: 20
batch_size: 256
lr: 0.0001
clip_grad : 10
patience : 20
log_steps : 50
split_ratio : 0.8

#중요
optimizer : adamw
scheduler : plateau
# plateau #linear_warmup
   
   
#use only in lgbm
lgbm:
    model_params: {
                    'objective': 'binary', # 이진 분류
                    'boosting_type': 'gbdt',
                    'metric': 'auc', # 평가 지표 설정
                    'feature_fraction': 0.8, # 원래 0.8 피처 샘플링 비율
                    'bagging_fraction': 0.8, # 원래 0.8 데이터 샘플링 비율
                    'bagging_freq': 1,
                    'n_estimators': 10000, # 트리 개수
                    'early_stopping_rounds': 100,
                    'seed': 42,
                    'verbose': -1,
                    'n_jobs': -1,
                    }
    
    verbose_eval : 100 #ori 100
    num_boost_round : 500
    early_stopping_rounds : 100
 

## LGBM feature enginnering 용 args
make_sharing_feature : True #extract statistics featrue from train + test(except last rows)
use_test_data : True #use test_data for train

## train && inference

run_train : True
run_inference : True

user_split_augmentation : True