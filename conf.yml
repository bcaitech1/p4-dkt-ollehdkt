work : 'server' # 'train_and_inference','tuning', 'server', 'test'

# # {% for q in questions %}
#             //     QUESTION_LIST.push({
#             //         assessmentItemID : "{{q.assessmentItemID}}",
#             //         testId : "{{q.testId}}",
#             //         KnowledgeTag : {{q.KnowledgeTag}},
#             //         real_answer : {{q.real_answer}},
#             //         img_url : "{{q.img_url}}",
#             //         q_content : "{{q.q_content}}"
#             //     })
#             // {% endfor %}

# 모델 : 일반화된 모델을 사용하고 싶으면, g를 붙일 것
# 서빙은 항상 lstm
model : lstm # {lstm, lstmattn, bert, lgbm, lstmroberta, lastquery_pre,lastquery_post, saint, lstmalbertattn,gLSTMConvATTN, gsaint, gsaintplus}

# 모델 서빙
# mlflow gui에 접속하여 방금 훈련을 마친 모델의 실험 기록을 확인 한우에, 모델 디렉토리를 복사

# 모델 디렉토리 예: file:///Users/Tom/projects/9rkd/mlruns/1/a69f1d42be0e404097c19e3d2cd7fb7a/artifacts/save_model
# 터미널에 아래 명령어 입력

# mlflow GUI와 포트가 겹치지 않도록 8890으로 실행

# mlflow models serve -m <saved-model-dir> --no-conda -h 0.0.0.0 -p 8890
# 주의 : mlflow ui -h 0.0.0.0 -p 8889
mlflow :
    using : True
    experiment : 'pytorch-test'

wandb :
    using: False
    project: DKT-slidding-gpt2

    ## 자신의 wandb 아이디를 적어주세요
    entity: sangki930
    tags: 
        - baseline

##main params
task_name: model-megabert-01
seed: 42
device: cuda

data_dir: /opt/ml/input/data/train_dataset
file_name: train_final copy.csv
# file_name : time_train.csv
# file_name : train_inter_time (1).csv
# file_name : train_time_finalfix.csv
# file_name : JH_train_v3.csv
test_file_name: test_data.csv
# test_file_name : time_test.csv
# test_file_name : test_inter_time (1).csv
# test_file_name : test_time_finalfix.csv
# test_file_name : JH_test_v3.csv

asset_dir: asset/
model_dir: models/
output_dir: output/

max_seq_len: 128
num_workers: 1

##K-fold params
use_kfold : False #n개의 fold를 이용하여 k-fold를 진행한다.
use_stratify : False
n_fold : 2
split_by_user : False #k-fold를 수행할 dataset을 user 기준으로 split

##모델
hidden_dim : 256
n_layers : 4
n_heads : 4
drop_out: 0.2

# 연속형 컬럼에 마스크를 적용할 것 인지
use_mask : True
use_roll : False

# Augmentation
window : False # 슬라이딩 윈도우를 사용할 것인지
stride : 128
shuffle : False
shuffle_n : 3

# transformers
concat_reverse : False

#train
n_epochs: 2
batch_size: 512
lr: 0.0001
clip_grad : 10
patience : 3
log_steps : 50
split_ratio : 0.8

#중요
optimizer : adamw
scheduler : plateau
# plateau #linear_warmup
   
   
#use only in lgbm
lgbm:
    model_params: {
                    'objective': 'binary', # 이진 분류
                    'boosting_type': 'gbdt',
                    'metric': 'auc', # 평가 지표 설정
                    'feature_fraction': 0.8, # 원래 0.8 피처 샘플링 비율
                    'bagging_fraction': 0.8, # 원래 0.8 데이터 샘플링 비율
                    'bagging_freq': 1,
                    'n_estimators': 10000, # 트리 개수
                    'early_stopping_rounds': 100,
                    'seed': 42,
                    'verbose': -1,
                    'n_jobs': -1,
                    }
    
    verbose_eval : 100 #ori 100
    num_boost_round : 500
    early_stopping_rounds : 100
 

## LGBM feature enginnering 용 args
make_sharing_feature : True #extract statistics featrue from train + test(except last rows)
use_test_data : True #use test_data for train

## train && inference

run_train : True
run_inference : True

user_split_augmentation : True